---
layout: default1
nav: links-ml
title: NLP
is_slide: 0
---
# Hugging Face
- Home
[(link)](https://huggingface.co/)
- Transformers
[(link)](https://huggingface.co/transformers/master/index.html)
- Transformers - git
[(link)](https://github.com/huggingface/transformers)
- Datasets
[(link)](https://huggingface.co/docs/datasets/)
- Models
[(link)](https://huggingface.co/models)
- Inside Hugging Face's Accelerate!
[(link)](https://wandb.ai/wandb_fc/pytorch-image-models/reports/Inside-Hugging-Face-s-Accelerate---Vmlldzo2MzgzNzA)
- Introducing HF Accelerate
[(link)](https://huggingface.co/blog/accelerate-library)
- Hugging Face on PyTorch / XLA TPUs: Faster and cheaper training
[(link)](https://huggingface.co/blog/pytorch-xla)
- Fine-Tune Wav2Vec2 for English ASR with HF-Transformers
[(link)](https://huggingface.co/blog/fine-tune-wav2vec2-english)
- The Partnership: Amazon SageMaker and Hugging Face
[(link)](https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face)
- Fine-tuning a model on a text classification task - colab
[(link)](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=rEJBSTyZIrIb)
- Fine-tuning a model on a token classification task
[(link)](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb)
- HF + Comet
[(link)](https://www.comet.ml/site/comet-hugging-face-integration/)
- GerPT2-large
[(link)](https://huggingface.co/benjamin/gerpt2-large)
- Multilingual Serverless XLM RoBERTa with HuggingFace, AWS Lambda
[(link)](https://towardsdatascience.com/multilingual-serverless-xlm-roberta-with-huggingface-aws-lambda-d69084f5deb1)
- Transformers-based Encoder-Decoder Models
[(link)](https://huggingface.co/blog/encoder-decoder)
- The ultimate guide to Transformer-based Encoder-Decoder Models (colab)
[(link)](https://colab.research.google.com/drive/18ZBlS4tSqSeTzZAVFxfpNDb_SrZfAOMf)
[(link)](https://colab.research.google.com/drive/1XpKHijllH11nAEdPcQvkpYHCVnQikm9G?usp=sharing)
[(link)](https://colab.research.google.com/drive/1HJhnWMFizEKKWEAb-k7QDBv4c03hXbCR?usp=sharing)
[(link)](https://colab.research.google.com/drive/1BFgJbPSeAQE7Wz0hgqyaDJj_4wkUrXgt?usp=sharing)
- Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT
[(link)](https://medium.com/huggingface/distilbert-8cf3380435b5)

# Attention, Transformer, Bert
- What Have Language Models Learned?
[(link)](https://pair.withgoogle.com/explorables/fill-in-the-blank/)
- Simple Transformers
[(link)](https://simpletransformers.ai/)
- Comparing Transformer Tokenizers
[(link)](https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955)
- Transformer Networks: A mathematical explanation why scaling the dot products leads to more stable gradients
[(link)](https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500)
- 10 Things You Need to Know About BERT and the Transformer Architecture That Are Reshaping the AI Landscape
[(link)](https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape)
- Bert Inner Workings
[(link)](https://gmihaila.medium.com/%EF%B8%8F-bert-inner-workings-1c3054cd1591)
- Summarization has gotten commoditized thanks to BERT
[(link)](https://towardsdatascience.com/summarization-has-gotten-commoditized-thanks-to-bert-9bb73f2d6922)
- Retrieval Augmented Generation with Huggingface Transformers and Ray
[(link)](https://medium.com/distributed-computing-with-ray/retrieval-augmented-generation-with-huggingface-transformers-and-ray-b09b56161b1e)
- How to Incorporate Tabular Data with HuggingFace Transformers
[(link)](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4)
- Extractive Text Summarization using Contextual Embeddings
[(link)](https://medium.com/nerd-for-tech/extractive-text-summarization-using-sentence-transformer-and-kmeans-clustering-algorithm-e942a6b33860)
- How not to use BERT for Document Ranking
[(link)](https://bergum.medium.com/how-not-to-use-bert-for-search-ranking-4586716428d9)
- Conversational Summarization with Natural Language Processing
[(link)](https://medium.com/rocket-mortgage-technology-blog/conversational-summarization-with-natural-language-processing-c073a6bcaa3a)
- Transformers
[(link)](https://tapanmittal.medium.com/transformers-9b38bb212fa0)
- ELECTRA — Addressing the flaws of BERT’s pre-training process
[(link)](https://medium.com/dair-ai/bert-is-extremely-inefficient-this-is-how-to-solve-it-688b09350f10)
- Encoder Decoder models in HuggingFace from (almost) scratch
[(link)](https://medium.com/@utk.is.here/encoder-decoder-models-in-huggingface-from-almost-scratch-c318cce098ae)
- Beyond BERT
[(link)](https://aditya-desai.medium.com/the-evolution-of-the-bert-based-model-81183694699c)
- Easy sentence similarity with BERT Sentence Embeddings using John Snow Labs NLU
[(link)](https://medium.com/spark-nlp/easy-sentence-similarity-with-bert-sentence-embeddings-using-john-snow-labs-nlu-ea078deb6ebf)
- TinyBERT — Size does matter, but how you train it can be more important
[(link)](https://medium.com/dair-ai/tinybert-size-does-matter-but-how-you-train-it-can-be-more-important-a5834831fa7d)
- ELECTRA: Pre-Training Text Encoders as Discriminators rather than Generators
[(link)](https://medium.com/towards-artificial-intelligence/electra-pre-training-text-encoders-as-discriminators-rather-than-generators-c5661f7ea0d5)
- Poor Man’s BERT — Exploring Pruning as an Alternative to Knowledge Distillation
[(link)](https://medium.com/dair-ai/poor-mans-bert-why-pruning-is-better-than-knowledge-distillation-%EF%B8%8F-f9652a1dc2bd)
- Data Extraction using Question Answering Systems
[(link)](https://medium.com/@foreseer/data-extraction-using-question-answering-systems-36850c0bf133)
- Understanding LongFormer’s Sliding Window Attention Mechanism
[(link)](https://blog.agolo.com/understanding-longformers-sliding-window-attention-mechanism-f5d61048a907)
- What Is The SMITH Algorithm?
[(link)](https://techguruseo.medium.com/what-is-the-smith-algorithm-6807be365e5e)
- BERT: Working with Long Inputs
[(link)](https://blog.agolo.com/bert-working-with-long-inputs-866479b9ac7e)
- XLNet outperforms BERT on several NLP Tasks
[(link)](https://medium.com/dair-ai/xlnet-outperforms-bert-on-several-nlp-tasks-9ec867bb563b)
- Text-to-Text Transfer Transformer
[(link)](https://medium.com/dataseries/text-to-text-transfer-transformer-e35dc28bae14)
- Transformer encoder - visualized
[(link)](https://github.com/mertensu/transformer-tutorial)
- Emergent linguistic structure in artificial neural networks trained by self-supervision
[(link)](https://www.pnas.org/content/117/48/30046.full)
- Understanding Language using XLNet with autoregressive pre-training
[(link)](https://medium.com/@zxiao2015/understanding-language-using-xlnet-with-autoregressive-pre-training-9c86e5bea443)
- Speeding up BERT
[(link)](https://blog.inten.to/speeding-up-bert-5528e18bb4ea)
- Pre-training BERT from scratch with cloud TPU
[(link)](https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379)
- Dissecting BERT Part 1: Understanding the Transformer
[(link)](https://medium.com/@mromerocalvo/dissecting-bert-part1-6dcf5360b07f)
- Understanding BERT Part 2: BERT Specifics
[(link)](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)
- deepset - bert
[(link)](https://deepset.ai/german-bert)
- deepset - farm
[(link)](https://github.com/deepset-ai/FARM)
- How GPT3 Works - Visualizations and Animations
[(link)](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
- The Annotated GPT-2
[(link)](https://amaarora.github.io/2020/02/18/annotatedGPT2.html)
- The Illustrated GPT-2 (Visualizing Transformer Language Models)
[(link)](https://jalammar.github.io/illustrated-gpt2/)
- A Visual Guide to Using BERT for the First Time
[(link)](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
- The Illustrated BERT, ELMo, and co.
[(link)](http://jalammar.github.io/illustrated-bert/)
- BERT - git google
[(link)](https://github.com/google-research/bert)
- The Illustrated Word2vec
[(link)](https://jalammar.github.io/illustrated-word2vec)
- The Annotated Encoder-Decoder with Attention
[(link)](https://bastings.github.io/annotated_encoder_decoder/)
- Seq2seq Models With Attention
[(link)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- How to code The Transformer in Pytorch
[(link)](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec)
- The Annotated Transformer
[(link)](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- The Illustrated Transformer
[(link)](http://jalammar.github.io/illustrated-transformer/)
- Transformers from scratch
[(link)](http://www.peterbloem.nl/blog/transformers)
- the transformer … “explained”?
[(link)](https://nostalgebraist.tumblr.com/post/185326092369/the-transformer-explained)
- PyTorch-Transformers
[(link)](https://github.com/huggingface/pytorch-transformers)
- Comprehensive Language Model Fine Tuning, Part 1
[(link)](https://www.ntentional.com/nlp/datasets/tokenization/processing/2020/10/09/comprehensive-datasets.html)
- Which flavor of BERT should you use for your QA task?
[(link)](https://towardsdatascience.com/which-flavor-of-bert-should-you-use-for-your-qa-task-6d6a0897fb24)
- Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT
[(link)](https://medium.com/huggingface/distilbert-8cf3380435b5)
- Fastai with Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)
[(link)](https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2)
- Using SimpleTransformers for Common NLP Applications
[(link)](https://wandb.ai/cayush/simpletransformers/reports/Using-SimpleTransformers-for-Common-NLP-Applications--Vmlldzo4Njk2NA)
- minGPT - karpathy
[(link)](https://github.com/karpathy/minGPT)
- A Quick Demo of Andrej Karpathy's minGPT Play Char Demo
[(link)](https://gist.github.com/morganmcg1/b2a26e213482d3355a3d3a64c91e94ac)
- BERT Text Classification Using Pytorch
[(link)](https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b)
- The Reformer - Pushing the limits of language modeling
[(link)](https://huggingface.co/blog/reformer)
- Visual Paper Summary: ALBERT (A Lite BERT)
[(link)](https://amitness.com/2020/02/albert-visual-summary/)
- GPT-2 and the Nature of Intelligence
[(link)](https://thegradient.pub/gpt2-and-the-nature-of-intelligence/)
- The Dark Secrets of BERT
[(link)](https://text-machine-lab.github.io/blog/2020/bert-secrets/)
- Encoder-decoders in Transformers: a hybrid pre-trained architecture for seq2seq
[(link)](https://medium.com/huggingface/encoder-decoders-in-transformers-a-hybrid-pre-trained-architecture-for-seq2seq-af4d7bf14bb8)
- Benchmarking Transformers: PyTorch and TensorFlow
[(link)](https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2)
- Transformers Hugginface GitHub
[(link)](https://github.com/huggingface/transformers)
- Transformers - A collection of resources to study Transformers in depth.
[(link)](https://github.com/sannykim/transformers)
- XLNet, ERNIE 2.0, And RoBERTa: What You Need To Know About New 2019 Transformer Models
[(link)](https://www.topbots.com/ai-nlp-research-big-language-models/)
- spaCy meets PyTorch-Transformers: Fine-tune BERT, XLNet and GPT-2
[(link)](https://explosion.ai/blog/spacy-pytorch-transformers)

# Misc
- Ultimate Guide To Text Similarity With Python - NewsCatcher
[(link)](https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python)
- Unsupervised Text Summarization using Sentence Embeddings
[(link)](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1)
- Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)
[(link)](https://medium.com/analytics-vidhya/text-mining-101-a-stepwise-introduction-to-topic-modeling-using-latent-semantic-analysis-using-add9c905efd9)
- An Introduction to Text Summarization using the TextRank Algorithm
[(link)](https://medium.com/analytics-vidhya/an-introduction-to-text-summarization-using-the-textrank-algorithm-with-python-implementation-2370c39d0c60)
- The Language Interpretability Tool (LIT)
[(link)](https://pair-code.github.io/lit/)
- A guide to language model sampling in AllenNLP
[(link)](https://medium.com/ai2-blog/a-guide-to-language-model-sampling-in-allennlp-3b1239274bc3)
- Going Beyond SQuAD (Part 1)
[(link)](https://medium.com/deepset-ai/going-beyond-squad-part-1-question-answering-in-different-languages-8eac6cf56f21)
- GEM Benchmark - for Natural Language Generation
[(link)](https://gem-benchmark.com/)
- Learn Natural Language Processing the practical way
[(link)](https://towardsdatascience.com/learn-nlp-the-practical-way-b854ce1035c4)
- GLUE Benchmark
[(link)](https://gluebenchmark.com/)
[(link)](https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html)
- Stanza – A Python NLP Package for Many Human Languages
[(link)](https://stanfordnlp.github.io/stanza/)
- nlp-tutorial
[(link)](https://github.com/graykode/nlp-tutorial)
- Self-host your HuggingFace Transformer NER model with Torchserve + Streamlit
[(link)](https://cceyda.github.io/blog/huggingface/torchserve/streamlit/ner/2020/10/09/huggingface_streamlit_serve.html)
- jiant is an NLP toolkit
[(link)](https://wp.nyu.edu/cilvr/2020/10/07/jiant-is-an-nlp-toolkit-introducing-jiant-2-0/)
- Transfer Learning for Natural Language Processing (Pact-Buch)
[(link)](https://www.manning.com/books/transfer-learning-for-natural-language-processing)
- NER-Papers
[(link)](https://github.com/pfliu-nlp/Named-Entity-Recognition-NER-Papers)
- Zero-Shot Learning in Modern NLP
[(link)](https://joeddav.github.io/blog/2020/05/29/ZSL.html)
- NLP's ImageNet moment has arrived
[(link)](http://ruder.io/nlp-imagenet/)
- NLP Year in Review — 2019
[(link)](https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19)
- Current Issues with Transfer Learning in NLP
[(link)](https://mohammadkhalifa.github.io/2019/09/06/Issues-With-Transfer-Learning-in-NLP/)
- 74 Summaries of Machine Learning and NLP Research
[(link)](http://www.marekrei.com/blog/74-summaries-of-machine-learning-and-nlp-research/)
- Evaluation Metrics for Language Modeling
[(link)](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)


# NER
- Named Entity Recognition — Clinical Data Extraction
[(link)](https://vishal-aiml164.medium.com/named-entity-recognition-clinical-data-extraction-9b089d91b27b)
- Training a spaCy NER Pipeline with Prodigy
[(link)](https://medium.com/analytics-vidhya/training-a-spacy-ner-pipeline-with-prodigy-ca58350cb868)
- Existing Tools for Named Entity Recognition
[(link)](http://mccormickml.com/2020/05/19/existing-ner-tools/)
- GermEval 2014 Named Entity Recognition Shared Task
[(link)](https://sites.google.com/site/germeval2014ner/ws-program)
- A Named Entity Recognition Shootout for German - pdf
[(link)](https://t.co/eQVFT0XR0r)
- Named Entity Recognition and the Road to Deep Learning
[(link)](http://nlp.town/blog/ner-and-the-road-to-deep-learning/)
- A Named-Entity Recognition Program based on Neural Networks and Easy to Use
[(link)](http://neuroner.com/)
- CRF Layer on the Top of BiLSTM 1
[(link)](https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/)
- CRF Layer on the Top of BiLSTM 2
[(link)](https://createmomo.github.io/2017/09/23/CRF_Layer_on_the_Top_of_BiLSTM_2/)
- CRF Layer on the Top of BiLSTM 3
[(link)](https://createmomo.github.io/2017/10/08/CRF-Layer-on-the-Top-of-BiLSTM-3/)
- CRF Layer on the Top of BiLSTM 4
[(link)](https://createmomo.github.io/2017/10/17/CRF-Layer-on-the-Top-of-BiLSTM-4/)
- CRF Layer on the Top of BiLSTM 5
[(link)](https://createmomo.github.io/2017/11/11/CRF-Layer-on-the-Top-of-BiLSTM-5/)
- CRF Layer on the Top of BiLSTM 6
[(link)](https://createmomo.github.io/2017/11/24/CRF-Layer-on-the-Top-of-BiLSTM-6/)
- CRF Layer on the Top of BiLSTM 7
[(link)](https://createmomo.github.io/2017/12/06/CRF-Layer-on-the-Top-of-BiLSTM-7/)
- CRF Layer on the Top of BiLSTM 8
[(link)](https://createmomo.github.io/2017/12/07/CRF-Layer-on-the-Top-of-BiLSTM-8/)
[(link)](https://github.com/ZhixiuYe/HSCRF-pytorch)
[(link)](https://arxiv.org/abs/1805.03838)

# Other
- link
- Haystack — Neural Question Answering At Scale
[(link)](https://github.com/deepset-ai/haystack)
- 5 NLP Libraries Everyone Should Know
[(link)](https://towardsdatascience.com/5-nlp-libraries-everyone-should-know-4f13f5263908)
- link
- The NLP Pandect
[(link)](https://github.com/ivan-bilan/The-NLP-Pandect)
- Text Summary Papers
[(link)](https://github.com/neulab/Text-Summarization-Papers)
- Transfer Learning in NLP - Folien Wolf Hugging Face
[(link)](https://docs.google.com/presentation/d/1LsUAhR_qIVbq6xH6Aw4ag8MGB_-UWfd0KoVhtTgye6o/edit#slide=id.g6e76c30798_0_0)
- SOTA NLP
[(link)](https://github.com/sebastianruder/NLP-progress/releases/tag/v0.1)
- Ruder NLP Newsletter
[(link)](http://newsletter.ruder.io/)
- Shuffling Paragraphs: Using Data Augmentation in NLP to Increase Accuracy
[(link)](https://medium.com/bcggamma/shuffling-paragraphs-using-data-augmentation-in-nlp-to-increase-accuracy-477388746bd9)
- The Conversational Intelligence Challenge 2 (ConvAI2)
[(link)](http://convai.io/)
- Workshop for Natural Language Processing Open Source Software
[(link)](https://nlposs.github.io/)
- How to Train your Own Model with NLTK and Stanford NER Tagger? (for English, French, German…)
[(link)](https://blog.sicara.com/train-ner-model-with-nltk-stanford-tagger-english-french-german-6d90573a9486)
- Supervised Word Vectors from Scratch in Rasa NLU
[(link)](https://medium.com/rasa-blog/supervised-word-vectors-from-scratch-in-rasa-nlu-6daf794efcd8)
- An overview of the NLP ecosystem in R
[(link)](http://www.bnosac.be/index.php/blog/87-an-overview-of-the-nlp-ecosystem-in-r-nlproc-textasdata)
- SNLI-decomposable-attention
[(link)](https://github.com/libowen2121/SNLI-decomposable-attention)
- A Review of the Neural History of Natural Language Processing
[(link)](http://blog.aylien.com/a-review-of-the-recent-history-of-natural-language-processing/)
- Eisenstein Buch
[(link)](https://github.com/jacobeisenstein/gt-nlp-class)
- Holy NLP! Understanding Part of Speech Tags, Dependency Parsing, and Named Entity Recognition
[(link)](https://pmbaumgartner.github.io/blog/holy-nlp)
- NLP Architect by Intel AI LAB
[(link)](https://github.com/NervanaSystems/nlp-architect)
- TutorialBank: Learning NLP Made Easier
[(link)](https://alex-fabbri.github.io/TutorialBank)
- Comparing Sentence Similarity Methods
[(link)](http://nlp.town/blog/sentence-similarity/)
- Text Embedding Models Contain Bias. Here's Why That Matters
[(link)](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html)
- The Natural Language Decathlon
[(link)](http://decanlp.com/)
- Joey NMT
[(link)](https://github.com/joeynmt/joeynmt)

# Spacy
- How to Fine-Tune BERT Transformer with spaCy 3
[(link)](https://walidamamou.medium.com/how-to-fine-tune-bert-transformer-with-spacy-3-6a90bfe57647)

